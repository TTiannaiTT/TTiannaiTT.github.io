---
---

@misc{tian2024eventaugmultifacetedspatiotemporaldata,
      abbr={arxiv},
      title={EventAug: Multifaceted Spatio-Temporal Data Augmentation Methods for Event-based Learning}, 
      author={Yukun Tian and Hao Chen and Yongjian Deng and Feihong Shen and Kepan Liu and Wei You and Ziyang Zhang},
      year={2024},
      eprint={2409.11813},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      pdf={EventAug.pdf},
      arxiv={2409.11813},
      url={https://arxiv.org/abs/2409.11813}, 
      preview={eventaug.jpg},
      abstract={The event camera has demonstrated significant success across a wide range of areas due to its low time latency and high dynamic range. However, the community faces challenges such as data deficiency and limited diversity, often resulting in over-fitting and inadequate feature learning. Notably, the exploration of data augmentation techniques in the event community remains scarce. This work aims to address this gap by introducing a systematic augmentation scheme named EventAug to enrich spatial-temporal diversity. In particular, we first propose Multi-scale Temporal Integration (MSTI) to diversify the motion speed of objects, then introduce Spatial-salient Event Mask (SSEM) and Temporal-salient Event Mask (TSEM) to enrich object variants. Our EventAug can facilitate models learning with richer motion patterns, object variants and local spatio-temporal relations, thus improving model robustness to varied moving speeds, occlusions, and action disruptions. Experiment results show that our augmentation method consistently yields significant improvements across different tasks and backbones (e.g., a 4.87% accuracy gain on DVS128 Gesture). Our code will be publicly available for this community.}
}
